{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b><p style=\"text-align:center;\">\n",
    "    <font size =\"8\" color =\"Green\">\n",
    "        Gangue (Silica Waste) Forecast in Flotation Concentrate\n",
    "    </font>\n",
    "</b>\n",
    "\n",
    "Mined ores are mostly mixtures of extractable minerals and nonvaluable material (gangue). Mineral processing (a.k.a. ore dressing, ore beneficiation) follows mining and prepares the ore for extraction of the valuable metal. A principal step in mineral processing is physical separation of the particles of valuable minerals from the gangue, to produce an enriched portion (concentrate) containing most of the valuable minerals, and a discard (tailing) containing predominantly the gangue.\n",
    "\n",
    "A separation of minerals by exploiting difference of surface properties (hydrophobicity) is called flotation. The reverse cationic flotation is commonly used to separate iron from silica. By adjusting the 'chemistry' of the pulp by adding various chemical reagents, iron minerals remain in the water and create sediment with a high concentration of iron (valuable minerals). At the same time, silica particles (gangue) attach to air bubbles and float to the surface.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img width=\"500\" alt=\"Reverse cationic flotation of iron ore\" src=\"https:\\\\github.com\\ginsaputra\\gangue-forecast-in-flotation-concentrate\\blob\\main\\reverse-cationic-flotation-iron-silica.png?raw=true\">\n",
    "\n",
    "Flotation concentrate is periodically sampled to determine its purity (i.e., *%valuable*, *%gangue*). Higher *%gangue* in the concentrate is undesirable as it indicates that most valuable minerals had gone into the tailings. Purity measurement is usually done in a lab and can take some time before process engineers can make any adjustments based on the results. A timely investigation of concentrate purity is, therefore, a fundamental aspect for the control and optimization of the flotation process.\n",
    "\n",
    "This notebook explores the application of deep learning to forecast **gangue (%silica)** in the flotation concentrate. The forecast will help process engineers assess purity of flotation concentrate and take corrective actions in advance. More specifically, the goal is to tackle the following tasks:\n",
    "- How many steps (hours) ahead can *%silica in concentrate* be forecasted?\n",
    "- Is it possible to forecast *%silica in concentrate* without using the data of *%iron in concentrate*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.19.5 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement libhdf5 (from versions: none)\n",
      "ERROR: No matching distribution found for libhdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from h5py) (1.19.5)\n",
      "Requirement already satisfied: six in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from h5py) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (4.0.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (3.19.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (0.23.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (1.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (59.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.10.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.10)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'export_saved_model' from 'tensorflow.python.keras.saving.saved_model' (C:\\Users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21016\\216458683.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Advanced activations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvanced_activations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvanced_activations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvanced_activations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mELU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\advanced_activations.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_summary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqueeze_or_expand_dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_gpu_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmulti_gpu_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\multi_gpu_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_distributed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaving\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport_saved_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_from_saved_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrace_model_call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'export_saved_model' from 'tensorflow.python.keras.saving.saved_model' (C:\\Users\\pdudar\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Load relevant libraries\n",
    "\n",
    "%pip install numpy==1.19.5\n",
    "\n",
    "%pip install libhdf5\n",
    "%pip install h5py\n",
    "%pip install tensorflow\n",
    "\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow \n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Plotly libraries\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Offline mode\n",
    "import plotly.offline as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Formatting options\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# Turn on execution time for JupyterLab\n",
    "try:\n",
    "    %reload_ext autotime\n",
    "except:\n",
    "    %pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flotation Data\n",
    "\n",
    "The dataset was obtained from a mineral processing plant separating silica from iron ore using the reverse cationic flotation method. Continuous process data were collected from 1 a.m. on March 10, 2017 to 11 p.m. on September 9, 2017.\n",
    "\n",
    "Each row of data consists of 23 measurements that can be categorized into four types:\n",
    "- raw materials (column 2-3);\n",
    "- environment variables (column 4-8);\n",
    "- process variables (column 9-22);\n",
    "- processed materials (column 23-24).\n",
    "\n",
    "Raw materials and processed materials were sampled on an hourly basis while the others were sampled every 20 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'C:/Users/pdudar/anaconda3/git/MiningProcessData/MiningProcess_Flotation_Plant_Database.csv'\n",
    "cols_renamed = [\n",
    "    'date',          # Timestamp of measurements, formatted YYYY-MM-DD HH:MM:SS\n",
    "    'feed_iron',     # %Iron (valuables) in the ore being fed into the flotation cell\n",
    "    'feed_silica',   # %Silica (gangue) in the ore being fed into the cell\n",
    "    'starch_flow',   # Amount of starch (reagent) added into the cell, measured in m^3\\h\n",
    "    'amina_flow',    # Amount of amina (reagent) added into the cell, measured in m^3\\h\n",
    "    'pulp_flow',     # Amount of ore pulp fed into the cell, measured in tonnes\\hour\n",
    "    'pulp_ph',       # Acidity\\alkalinity of ore pulp on a scale of 0-14\n",
    "    'pulp_density',  # Amount of ore in the pulp, between 1-3 kg\\cm^3\n",
    "    'air_col1',      # Volume of air injected into the cell, measured in Nm3\\h\n",
    "    'air_col2',\n",
    "    'air_col3',\n",
    "    'air_col4',\n",
    "    'air_col5',\n",
    "    'air_col6',\n",
    "    'air_col7',\n",
    "    'level_col1',    # Froth height in the cell, measured in mm\n",
    "    'level_col2',\n",
    "    'level_col3',\n",
    "    'level_col4',\n",
    "    'level_col5',\n",
    "    'level_col6',\n",
    "    'level_col7',\n",
    "    'conc_iron',     # Lab measurement: %Iron in the end of flotation process\n",
    "    'conc_silica']   # Lab measurement: %Silica in the end of flotation process\n",
    "\n",
    "df = pd.read_csv(\n",
    "                file,\n",
    "                header=0,\n",
    "                names=cols_renamed,\n",
    "                parse_dates=['date'],\n",
    "                infer_datetime_format=True,\n",
    "                decimal=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols = df.columns[1:]\n",
    "\n",
    "for col in float_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce', downcast='float')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feed_iron_title'] = 'Iron Feed'\n",
    "df['feed_silica_title'] = 'Silica Feed'\n",
    "df['conc_iron_title'] = 'Iron Concentration'\n",
    "df['conc_silica_title'] = 'Silica Concentration'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Time-Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column `date` is resampled to reduce the frequency of our time-series data into an **hourly basis**. This is achieved by selecting only the first measurements of each hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample data to 15 min basis\n",
    "df = df.set_index('date').resample('H').first()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the data, 318 rows containing missing values are found between `2017-03-16 06:00` to `2017-03-29 11:00`. Missing values introduce discontinuity in the time-series data and can be detrimental to our forecast. Therefore, only the data starting from `2017-03-29 12:00` will be further used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = df[df.isna().any(axis=1)]  # Check for missing values\n",
    "print(f'Total rows with NaNs: {nans.shape[0]}\\n')\n",
    "nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data with time discontinuity\n",
    "df = df['2017-03-29 12:00:00':]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sampling on 15 minute intervals there are clear gaps in data on an hourly interval.\n",
    "\n",
    "We can try to fill these gaps by interpolating between the values. This is not a guarantee of what is actually happening, but we'll \n",
    "see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.interpolate(axis=0, method='linear')\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows mineral content before (i.e., in the feed) and after flotation process (in the concentrate). As can be observed from the figure, the purpose of flotation is to increase recovery of iron mineral while reducing the gangue (silica).\n",
    "\n",
    "During some periods (e.g., May 13 to June 13), mineral content in the feed was constant but the resulting content in the concentrate fluctuated. This suggests that *%iron* and *%silica in concentrate* are not solely governed by the content of raw materials but other parameters as well (i.e., environment, process variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ['feed_iron', 'feed_silica', 'conc_iron', 'conc_silica']\n",
    "palette = ['#FB6542', '#FFBB00', '#3F681C', '#375E97']\n",
    "\n",
    "# Plot mineral content before and after flotation\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(18,6))\n",
    "for pct, color in zip(content, palette):\n",
    "    ax.plot(df.index.values, pct, data=df, color=color)\n",
    "ax.set_title('Mineral content in feed and concentrate',\n",
    "             loc='left', weight='bold', size=16)\n",
    "ax.set_ylabel('% Mineral')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.legend(loc='center left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with secondary y-axis\n",
    "from plotly.subplots import make_subplots\n",
    "subfig = make_subplots(specs=[[{\"secondary_y\": True}]], shared_yaxes=True)\n",
    "\n",
    "# Generate a Plotly TimeSeries chart for each monitoring station\n",
    "\n",
    "fig = px.line(df, \n",
    "              x = df.index.values,\n",
    "              y = 'feed_iron',\n",
    "              title = 'Mineral content in feed and concentrate',\n",
    "              hover_name = 'feed_iron_title', \n",
    "              hover_data = ['feed_iron'],\n",
    "               render_mode=\"webgl\")\n",
    "             \n",
    "\n",
    "fig2 = px.line(df, \n",
    "              x = df.index.values,\n",
    "              y = 'feed_silica',\n",
    "              hover_name = 'feed_silica_title', \n",
    "              hover_data = ['feed_silica'],\n",
    "               render_mode=\"webgl\")\n",
    "\n",
    "\n",
    "fig3 = px.line(df, \n",
    "              x = df.index.values,\n",
    "              y = 'conc_iron',\n",
    "              hover_name = 'conc_iron_title', \n",
    "              hover_data = ['conc_iron'],\n",
    "               render_mode=\"webgl\")\n",
    "             \n",
    "\n",
    "fig4 = px.line(df, \n",
    "              x = df.index.values,\n",
    "              y = 'conc_silica',\n",
    "              hover_name = 'conc_silica_title', \n",
    "              hover_data = ['conc_silica'],\n",
    "               render_mode=\"webgl\")\n",
    "\n",
    "fig.update_traces(yaxis=\"y1\")\n",
    "fig2.update_traces(yaxis=\"y1\")\n",
    "fig3.update_traces(yaxis=\"y1\")\n",
    "fig4.update_traces(yaxis=\"y1\")\n",
    "\n",
    "subfig.add_traces(fig.data + fig2.data + fig3.data + fig4.data)\n",
    "\n",
    "subfig.layout.xaxis.title=\"DATE\"\n",
    "subfig.layout.yaxis.title=\"MINERAL CONCENTRATION (%)\"\n",
    "subfig.layout.yaxis.type=\"linear\"\n",
    "\n",
    "#subfig.layout.yaxis2.type=\"linear\"\n",
    "#subfig.layout.yaxis2.title=\"UPPER AND LOWER GUIDELINE RECCOMENDATIONS\"\n",
    "\n",
    "# recoloring is necessary otherwise lines from fig und fig2 would share each color\n",
    "# e.g. Linear-, Log- = blue; Linear+, Log+ = red... we don't want this\n",
    "subfig.for_each_trace(lambda t: t.update(line=dict(color=t.marker.color))) \n",
    "\n",
    "# Add figure title\n",
    "subfig.update_layout(title_text = 'Mineral content in feed and concentrate: Time Series with Range Slider')\n",
    "\n",
    "subfig.update_layout(yaxis=dict(title='(%) MINERAL CONCENTRATION', tick0=0, anchor='x', rangemode='normal'))\n",
    "#subfig.update_layout(yaxis2=dict(title='UPPER AND LOWER GUIDELINE RECCOMENDATIONS',\n",
    "                                # tick0=0, side='right', anchor='x', rangemode='normal'))\n",
    "\n",
    "# Set y-axes titles\n",
    "subfig.update_yaxes(title_text=\"<b>CONCENTRATION (%)</b>\", secondary_y=False, autorange=True)\n",
    "\n",
    "subfig.update_xaxes(\n",
    "    rangeslider_visible=True,\n",
    "    rangeselector=dict(\n",
    "        buttons=list([\n",
    "            dict(count=1, label=\"1DAY\", step=\"day\", stepmode=\"backward\"),\n",
    "            dict(count=6, label=\"6MNTH\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=1, label=\"1MNTH\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=6, label=\"6MNTH\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "            dict(count=1, label=\"1YR\", step=\"year\", stepmode=\"backward\"),\n",
    "            dict(step=\"all\")\n",
    "        ])\n",
    "    )\n",
    ")\n",
    "\n",
    "# Chart options\n",
    "subfig.update_layout(plot_bgcolor = \"RGB(45,45,48)\")  #HEX 2d2d30 \n",
    "subfig.update_layout(paper_bgcolor = \"RGB(37,37,38)\") #HEX 252526\n",
    "\n",
    "subfig.update_layout(\n",
    "    font_color=\"RGB(131,148,150)\",  \t#839496\n",
    "    title_font_color=\"RGB(131,148,150)\",  \t#839496\n",
    "    legend_title_font_color=\"RGB(131,148,150)\"\n",
    "    ) #839496\n",
    "\n",
    "subfig.update_layout(height=600, width=1200)\n",
    "\n",
    "# Append wq station names to .html files\n",
    "subfig.write_html(\"Mineral content in feed and concentrate: Time Series with Range Slider.html\", include_plotlyjs='cdn')\n",
    "subfig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `conc_iron` is dropped from the dataframe because we want to look at forecasting **%silica in concentrate** without including **iron in concentrate** as a feature. The data are then normalized as they have different units and scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "cols = ['conc_silica', 'feed_iron', 'feed_silica',\n",
    "        'starch_flow', 'amina_flow', 'pulp_flow',\n",
    "        'pulp_ph', 'pulp_density', 'air_col1',\n",
    "        'air_col2', 'air_col3', 'air_col4',\n",
    "        'air_col5', 'air_col6', 'air_col7',\n",
    "        'level_col1', 'level_col2', 'level_col3',\n",
    "        'level_col4', 'level_col5', 'level_col6', \n",
    "        'level_col7', 'conc_iron']\n",
    "\n",
    "cols.insert(0, cols.pop(         # Moving target `conc_silica` to the front\n",
    "    cols.index('conc_silica')))  # Not necessary but I prefer to do so\n",
    "df = df.loc[:, cols]\n",
    "df.to_csv('Flotation_Dataset_by_Hour.csv')  # For safekeeping\n",
    "\n",
    "# Drop `conc_iron` then normalize all data\n",
    "values = df.drop('conc_iron', axis=1).values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "scaled[0]  # Show first element of the array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing stage involves framing the dataset as a supervised learning problem where we forecast the **%silica in concentrate** \n",
    "at the current hour (**t**) given the parameters (i.e., raw materials, environment, process) in prior time steps **(t-n)**). \n",
    "\n",
    "We transform the dataset using the `series_to_supervised()` function below, which was adapted from the blog [Machine Learning Mastery](https:\\\\machinelearningmastery.com\\convert-time-series-supervised-learning-problem-python\\) by Jason Brownlee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, drop_nan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        drop_nan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    for i in range(n_in, 0, -1):   # Input sequence (t-n, ... t-1)\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        \n",
    "    for i in range(0, n_out):      # Forecast sequence (t, t+1, ... t+n)\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "            \n",
    "    agg = pd.concat(cols, axis=1)  # Put it all together\n",
    "    agg.columns = names\n",
    "    if drop_nan:                    # Drop rows with NaN values\n",
    "        agg.dropna(inplace=True)\n",
    "        \n",
    "    # Drop columns we don't want to predict\n",
    "    drop_cols = ['var'+ str(i) +'(t)' for i in range(2,23)]\n",
    "    agg.drop(columns=drop_cols, axis=1, inplace=True)   \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reframed = series_to_supervised(scaled, n_in=1, n_out=1)\n",
    "reframed  # Show reframed dataset \n",
    "\n",
    "# t-1 = 0.25 hrs or 15 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column `var1(t-1)` until `var22(t-1)` are our features and inputs. Measured values of the inputs are lagged 1 hour before the target `var1(t)` (*%silica in concentrate* at time *t*).\n",
    "\n",
    "Transformed data is further split into three sets: training (60%), validation (20%), and testing (20%). Flotation data in the period between `2017-03-29 12:00` and `2017-08-08 00:00` is used for training\\validation purpose, totaling 3157 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an autoregression model to predict the %silica concentrate (var1(t)) using measured values lagged 30 minutes (var1(t-0.5)) as features/inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 22             # Number of inputs for forecast\n",
    "n_hours = 1             # Number of hours with which to lag features (1 = 1 hour, 0.5 = 30 mins, 0.25 = 15 mins etc)\n",
    "n_obs = n_hours*n_features\n",
    "\n",
    "# Define row size of each split\n",
    "n_train = int(np.round(len(reframed) * .60))\n",
    "n_valid = int(np.round(len(reframed) * .20))\n",
    "n_test = int(np.round(len(reframed) * .20))\n",
    "\n",
    "# Split dataset by row size\n",
    "values = reframed.values\n",
    "train = values[:n_train, :]\n",
    "valid = values[n_train:(n_train + n_valid), :]\n",
    "test = values[(n_train + n_valid):, :]\n",
    "\n",
    "# Each set further split into inputs\\features (X) and output (y)\n",
    "train_X, train_y = train[:, :n_obs], train[:, -1]\n",
    "valid_X, valid_y = valid[:, :n_obs], valid[:, -1]\n",
    "test_X, test_y = test[:, :n_obs], test[:, -1]\n",
    "\n",
    "# Reshape inputs (X) to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "valid_X = valid_X.reshape((valid_X.shape[0], n_hours, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "\n",
    "print(  # Show the final shape of each set\n",
    "    f'Training set: {train_X.shape}, {train_y.shape}',\n",
    "    f'\\nValidation set: {valid_X.shape}, {valid_y.shape}',\n",
    "    f'\\nTesting set: {test_X.shape}, {test_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting gangue in flotation concentrate is a time-series related problem. A variation of the recurrent neural networks (RNN), called the long short-term memory (LSTM), is a deep learning approach that can be implemented to solve the problem.\n",
    "> *LSTMs have an edge over conventional feed-forward neural networks and RNN in many ways. This is because of their property of selectively remembering patterns for long durations of time.* -[Analytics Vidhya](https:\\\\www.analyticsvidhya.com\\blog\\2017\\12\\fundamentals-of-deep-learning-introduction-to-lstm\\)\n",
    "\n",
    "Model construction and training is done with [Keras](https:\\\\tensorflow.org\\api_docs\\python\\tf\\keras), a Python deep learning library. The model is made up of two [LSTM](https:\\\\www.tensorflow.org\\api_docs\\python\\tf\\keras\\layers\\LSTM) layers, each with 16 memory cells, and followed by a fully-connected ([Dense](https:\\\\www.tensorflow.org\\api_docs\\python\\tf\\keras\\layers\\Dense)) layer. In training, the data are grouped into mini batches of 16 training samples. The training process is run for 100 epochs (one epoch is the number of passes needed to complete the entire training samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([   # Define a sequential model\n",
    "    LSTM(units=16,\n",
    "         return_sequences=True,\n",
    "         input_shape=(train_X.shape[1],\n",
    "                      train_X.shape[2])),\n",
    "    LSTM(units=16),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(\n",
    "    loss='mae',        # Mean absolute error\n",
    "    optimizer='adam')  # Learning rate = 0.001\n",
    "model.summary()        # Display model's architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model (Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:/Users/pdudar/anaconda3/git/MiningProcessData/')\n",
    "\n",
    "# Load tensorboard notebook extension\n",
    "%reload_ext tensorboard\n",
    "\n",
    "# Add TensorBoard callback variables\n",
    "logdir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tensorflow.keras.callbacks.TensorBoard(log_dir=logdir, \n",
    "                                                              histogram_freq=1,\n",
    "                                                              profile_batch = 100000000)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('LSTM_Flotation_Gangue.hdf5/saved_model.pb')\n",
    "\n",
    "history = model.fit(   # Fit on training data\n",
    "            train_X,\n",
    "            train_y,\n",
    "            epochs=30,\n",
    "            batch_size=16,\n",
    "            validation_data=(  # Supply validation data\n",
    "                valid_X, #X_test\n",
    "                valid_y), #y_test\n",
    "            verbose=2,\n",
    "            shuffle=False,\n",
    "            callbacks=([model_checkpoint])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract losses from the training history and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract losses from training history\n",
    "train_loss = history.history['loss']\n",
    "valid_loss = history.history['val_loss']\n",
    "\n",
    "# Plot learning curves\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "ax.plot(train_loss, color=palette[0], label='Training')\n",
    "ax.plot(valid_loss, color=palette[2], label='Validation')\n",
    "ax.set_title('Learning Curves', loc='left', weight='bold', size=16)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (Mean Absolute Error)')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.legend(loc='center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above indicates that the model exhibits a good fit where training and validation losses decrease to a point of stability and there is minimal gap between the two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best training loss = {min(train_loss):.4f}',\n",
    "      f'at epoch {train_loss.index(min(train_loss))}',\n",
    "      f'\\nBest validation loss = {min(valid_loss):.4f}',\n",
    "      f'at epoch {valid_loss.index(min(valid_loss))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting is performed using the trained model and on the data that was not included during training\\validation process. Forecasts and actual values are inverted back into their original scales before calculating error scores for the model. Two error metrics are considered in the forecast evaluation: (1) mean absolute error, and (2) root mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction using test features\n",
    "yhat = model.predict(test_X)\n",
    "\n",
    "# Reshape test data\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "\n",
    "# Invert scaling for forecasts\n",
    "inv_yhat = np.concatenate((yhat, test_X[:, -(n_features-1):]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "\n",
    "# Invert scaling for actual values\n",
    "inv_y = np.concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "\n",
    "# Calculate error scores\n",
    "mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print(32*'-'+'\\nFORECAST EVALUATION' + '\\n' + 32 * '-',\n",
    "      f'\\nMean absolute error    : {mae:.4f}',\n",
    "      f'\\nRoot mean squared error: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Calc z-score for inv_y (% Silica Concentrate) Actual Values\n",
    "z = zscore(inv_y)\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define date as x-axis\n",
    "test_date = df.index[-test_y.shape[0]:]\n",
    "\n",
    "# Define the 95% Confidence interval (Z-value 1.96)\n",
    "ci = z * (np.std(inv_y) / np.mean(inv_y))\n",
    "\n",
    "# Plot actual values and forecasts\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(18,6))\n",
    "ax.plot(test_date, inv_y, color=palette[1], label='Actual Value')\n",
    "ax.plot(test_date, inv_yhat, color=palette[2], label='Forecast')\n",
    "ax.fill_between(test_date, (inv_y-ci), (inv_y+ci), color=palette[3],\n",
    "                alpha=.1, label='95(%) Confidence Interval')\n",
    "ax.set_title('(%) Silica in Concentrate: Actual Values and Forecasts by LSTM',\n",
    "             loc='left', weight='bold', size=16)\n",
    "ax.set_ylabel('Silica in Concentrate (%)')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "subfig = make_subplots(specs=[[{\"secondary_y\": True}]], shared_yaxes=True)\n",
    "\n",
    "fig = go.Figure([\n",
    "    go.Scatter(\n",
    "        name='(%) Silica in Conc: Actual Value',\n",
    "        x=test_date,\n",
    "        y=inv_y,\n",
    "        mode='lines',\n",
    "        line=dict(color='rgb(24, 48, 96)')\n",
    "    ),\n",
    "    \n",
    "    go.Scatter(\n",
    "        name='(%) Silica in Conc: Forecast Value',\n",
    "        x=test_date,\n",
    "        y=inv_yhat,\n",
    "        mode='lines',\n",
    "        line=dict(color='rgb(48, 96, 192)')\n",
    "    ),\n",
    "    \n",
    "    go.Scatter(\n",
    "        name='95 perc CI: Upper Bound',\n",
    "        x=test_date,\n",
    "        y=inv_y+ci,\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        showlegend=True\n",
    "    ),\n",
    "    \n",
    "    go.Scatter(\n",
    "        name='95 perc CI: Lower Bound',\n",
    "        x=test_date,\n",
    "        y=inv_y-ci,\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        fillcolor='rgba(68, 68, 68, 0.3)',\n",
    "        fill='tonexty',\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis_title='Silica in Conc (%)',\n",
    "    title='(%) Silica in Conc: Actual Values and Forecasts by LSTM',\n",
    "    hovermode=\"x\",\n",
    "    height=500, width=1500\n",
    ")\n",
    "\n",
    "subfig.add_traces(fig.data)\n",
    "\n",
    "subfig.layout.xaxis.title=\"Date\"\n",
    "subfig.layout.yaxis.title=\"Silica in Concentrate (%)\"\n",
    "subfig.layout.yaxis.type=\"linear\"\n",
    "\n",
    "# recoloring is necessary otherwise lines from fig und fig2 would share each color\n",
    "# e.g. Linear-, Log- = blue; Linear+, Log+ = red... we don't want this\n",
    "subfig.for_each_trace(lambda t: t.update(line=dict(color=t.marker.color))) \n",
    "\n",
    "# Add figure title\n",
    "subfig.update_layout(title_text = '(%) Silica in Concentrate: Actual Values and Forecasts by LSTM: Time Series with Range Slider')\n",
    "subfig.update_layout(yaxis=dict(title='Silica in Concentrate (%)', tick0=0, anchor='x', rangemode='normal'))\n",
    "\n",
    "# Set y-axes titles\n",
    "subfig.update_yaxes(title_text=\"<b>Silica in Concentrate (%)</b>\", secondary_y=False, autorange=True)\n",
    "\n",
    "subfig.update_xaxes(\n",
    "    rangeslider_visible=True,\n",
    "    rangeselector=dict(\n",
    "        buttons=list([\n",
    "            dict(count=1, label=\"1DAY\", step=\"day\", stepmode=\"backward\"),\n",
    "            dict(count=7, label=\"1WEEK\", step=\"day\", stepmode=\"backward\"),\n",
    "            dict(count=1, label=\"1MNTH\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=6, label=\"6MNTH\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "            dict(count=1, label=\"1YR\", step=\"year\", stepmode=\"backward\"),\n",
    "            dict(step=\"all\")\n",
    "        ])\n",
    "    )\n",
    ")\n",
    "\n",
    "subfig.update_layout(height=600, width=1200)\n",
    "\n",
    "# Chart options\n",
    "subfig.update_layout(plot_bgcolor = \"RGB(45,45,48)\")  #HEX 2d2d30 \n",
    "subfig.update_layout(paper_bgcolor = \"RGB(37,37,38)\") #HEX 252526\n",
    "\n",
    "subfig.update_layout(\n",
    "    font_color=\"RGB(131,148,150)\",  \t#839496\n",
    "    title_font_color=\"RGB(131,148,150)\",  \t#839496\n",
    "    legend_title_font_color=\"RGB(131,148,150)\"\n",
    "    ) #839496\n",
    "\n",
    "subfig.update_layout(height=600, width=1200)\n",
    "\n",
    "# Append wq station names to .html files\n",
    "subfig.write_html(\"SilicaInConcentrate_ActualValues_ForecastLSTM.html\", include_plotlyjs='cdn')\n",
    "subfig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison of forecasts and actual values of *%silica in concentrate* is displayed above for the period between 1 a.m. August 8, 2017 and 11 p.m. September 9, 2017. The forecasts largely follow the pattern of actual values and contained within the 95% confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecast Comparison with Random Forest\n",
    "A random forest regressor, in comparison to LSTM, generates greater error and may not perform as well for long-term forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reframe data by lagging features by 1 hour\n",
    "rf_values = df.drop(['CONC_IRON', 'IRON_FEED_TITLE',\n",
    "                'SILICA_FEED_TITLE',\n",
    "                'CONC_IRON_TITLE',\n",
    "                'CONC_SILICA_TITLE'], axis=1).values\n",
    "rf_reframed = series_to_supervised(rf_values, n_in=1, n_out=1)\n",
    "\n",
    "# Define features and target\n",
    "rf_X = rf_reframed.values[:, :-1]\n",
    "rf_y = rf_reframed.values[:, -1]\n",
    "\n",
    "# Split data into train/test sets (80/20)\n",
    "rf_train_X, rf_test_X, rf_train_y, rf_test_y = train_test_split(\n",
    "    rf_X, rf_y, test_size=.20, random_state=0, shuffle=False)\n",
    "\n",
    "# Normalize features\n",
    "rf_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "rf_train_X = rf_scaler.fit_transform(rf_train_X)\n",
    "rf_test_X = rf_scaler.transform(rf_test_X)\n",
    "\n",
    "# Instantiate regressor\n",
    "forest = RandomForestRegressor(random_state=1234)\n",
    "\n",
    "# Fit model on training data\n",
    "forest.fit(rf_train_X, rf_train_y)\n",
    "\n",
    "# Make prediction using trained model\n",
    "rf_yhat = forest.predict(rf_test_X)\n",
    "\n",
    "# Calculate error scores\n",
    "rf_mae = mean_absolute_error(rf_test_y, rf_yhat)\n",
    "rf_rmse = np.sqrt(mean_squared_error(rf_test_y, rf_yhat))\n",
    "print(32*'-'+'\\nFORECAST EVALUATION'+'\\n'+32*'-',\n",
    "      f'\\nMean absolute error    : {rf_mae:.4f}',\n",
    "      f'\\nRoot mean squared error: {rf_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual values and forecasts\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(18,6))\n",
    "ax.plot(test_date, rf_test_y, color=palette[1], label='Actual Value')\n",
    "ax.plot(test_date, rf_yhat, color=palette[2], label='Forecast')\n",
    "ax.fill_between(test_date, (rf_test_y-ci), (rf_test_y+ci), color=palette[3],\n",
    "                alpha=.1, label='95% Confidence Interval')\n",
    "ax.set_title('(%) Silica in Concentrate: Actual Values and Forecasts by Random Forest',\n",
    "             loc='left', weight='bold', size=16)\n",
    "ax.set_ylabel('Silica in Concentrate (%)')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "subfig = make_subplots(specs=[[{\"secondary_y\": True}]], shared_yaxes=True)\n",
    "\n",
    "fig = go.Figure([\n",
    "    go.Scatter(\n",
    "        name='(%) Silica in Concentrate: Actual Value',\n",
    "        x=test_date,\n",
    "        y=rf_test_y,\n",
    "        mode='lines',\n",
    "        line=dict(color='rgb(31, 119, 180)')\n",
    "    ),\n",
    "    \n",
    "    go.Scatter(\n",
    "        name='(%) Silica in Concentrate: Forecast Value',\n",
    "        x=test_date,\n",
    "        y=rf_yhat,\n",
    "        mode='lines',\n",
    "        line=dict(color='rgb(62, 100, 240)')\n",
    "    ),\n",
    "    \n",
    "    go.Scatter(\n",
    "        name='95% CI: Upper Bound',\n",
    "        x=test_date,\n",
    "        y=rf_test_y+ci,\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        showlegend=True\n",
    "    ),\n",
    "    \n",
    "    go.Scatter(\n",
    "        name='95% CI: Lower Bound',\n",
    "        x=test_date,\n",
    "        y=rf_test_y-ci,\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        fillcolor='rgba(68, 68, 68, 0.3)',\n",
    "        fill='tonexty',\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis_title='Silica in Concentrate (%)',\n",
    "    title=' (%) Silica in Concentrate: Actual Values and Forecasts by Random Forest',\n",
    "    hovermode=\"x\",\n",
    "    height=500, width=1500\n",
    ")\n",
    "\n",
    "subfig.add_traces(fig.data)\n",
    "\n",
    "subfig.layout.xaxis.title=\"Date\"\n",
    "subfig.layout.yaxis.title=\"Silica in Concentrate (%)\"\n",
    "subfig.layout.yaxis.type=\"linear\"\n",
    "\n",
    "# recoloring is necessary otherwise lines from fig und fig2 would share each color\n",
    "# e.g. Linear-, Log- = blue; Linear+, Log+ = red... we don't want this\n",
    "subfig.for_each_trace(lambda t: t.update(line=dict(color=t.marker.color))) \n",
    "\n",
    "# Add figure title\n",
    "subfig.update_layout(title_text = '% Silica in Concentrate: Actual Values and Forecasts by Random Forest: Time Series with Range Slider')\n",
    "\n",
    "subfig.update_layout(yaxis=dict(title='Silica in Concentrate (%)', tick0=0, anchor='x', rangemode='normal'))\n",
    "#subfig.update_layout(yaxis2=dict(title='UPPER AND LOWER GUIDELINE RECCOMENDATIONS',\n",
    "                                # tick0=0, side='right', anchor='x', rangemode='normal'))\n",
    "\n",
    "# Set y-axes titles\n",
    "subfig.update_yaxes(title_text=\"<b>Silica in Concentrate (%)</b>\", secondary_y=False, autorange=True)\n",
    "\n",
    "subfig.update_xaxes(\n",
    "    rangeslider_visible=True,\n",
    "    rangeselector=dict(\n",
    "        buttons=list([\n",
    "            dict(count=7, label=\"1WEEK\", step=\"day\", stepmode=\"backward\"),\n",
    "            dict(count=14, label=\"2WEEK\", step=\"day\", stepmode=\"backward\"),\n",
    "            dict(count=1, label=\"1MNTH\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=6, label=\"6MNTH\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "            dict(count=1, label=\"1YR\", step=\"year\", stepmode=\"backward\"),\n",
    "            dict(step=\"all\")\n",
    "        ])\n",
    "    )\n",
    ")\n",
    "\n",
    "subfig.update_layout(height=600, width=1200)\n",
    "\n",
    "# Chart options\n",
    "subfig.update_layout(plot_bgcolor = \"RGB(45,45,48)\")  #HEX 2d2d30 \n",
    "subfig.update_layout(paper_bgcolor = \"RGB(37,37,38)\") #HEX 252526\n",
    "\n",
    "subfig.update_layout(\n",
    "    font_color=\"RGB(131,148,150)\",  \t#839496\n",
    "    title_font_color=\"RGB(131,148,150)\",  \t#839496\n",
    "    legend_title_font_color=\"RGB(131,148,150)\"\n",
    "    ) #839496\n",
    "\n",
    "subfig.update_layout(height=600, width=1200)\n",
    "\n",
    "# Append wq station names to .html files\n",
    "subfig.write_html(\"Silica in Concentrate_ActualValuesForecastRandomForest.html\", include_plotlyjs='cdn')\n",
    "subfig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "A deep learning approach using LSTM was implemented to forecast gangue content in flotation concentrate. Excluding *%iron in concentrate* from the features, *%silica in concentrate* were forecasted one hour ahead and with an error below 1 (based on RMSE, MAE). As the dataset owner stated in [this post](https:\\\\www.kaggle.com\\rogerbellavista\\randomforestregressor-mae-0-0922-rmse-0-2314#434654), MAE and RMSE of 10.2 is a satisfactory result\n",
    "for a predictive process. The forecasts are a  promising method for process engineers to assess concentrate purity and take corrective actions in advance, especially when concentrate purity deviates from acceptable values.\n",
    "\n",
    "This project is an example of a Continuous Improvment (CI) process with potential to optimize mining processes. Having a real time log and backlog\n",
    "of gangue content can help process engineers and decision makers identify problems in the mining process.\n",
    "\n",
    "Finally, although LSTM implementation in this notebook has met the objectives, it will benefit from further exploration:\n",
    "- Forecasting with smaller lag timesteps. For example, a 30-minute lag for the features and inputs.\n",
    "- Analysis of feature importance in order to understand which parameters of the flotation process greatly affect *%silica in concentrate*. This ensures that the important parameters are adjusted accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf4d54fbd1aa67388c0b0b70ec3753651b0e16d7eeb31ef24b0b3bf91c5c03b6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
